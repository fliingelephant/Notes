% !TEX program = xelatex

\documentclass[UTF8]{ctexart}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{bm}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{hyperref}

\newcommand{\btheta}{\boldsymbol{\theta}}
\geometry{a4paper, margin=2cm}
\title{量子 Fisher 信息矩阵 vs 经典 Fisher 信息矩阵\\[0.5em]\large 从 KL 散度到 Fubini-Study 距离，从自然梯度到随机重构的自洽推导}
\author{}
\date{}
\begin{document}
\maketitle

\begin{abstract}
经典信息几何中，KL 散度在小参数扰动下的二阶展开诱导出 Fisher-Rao 度量，其度量张量即 Fisher 信息矩阵（FIM）；对应的"最速下降"给出自然梯度法。量子纯态的物理态是射线（ray），存在全局相位与整体归一化的规范冗余，因而不能直接把"对输出的欧氏内积/协方差"当作度量；正确的距离应定义在射影 Hilbert 空间上，即 Fubini-Study（FS）距离。FS 距离的二阶展开给出量子 Fisher 信息矩阵（又称量子几何张量，QGT）：其实部是 FS 黎曼度量，虚部是 Berry 曲率。进一步地，在以 FS 范数约束的最速下降问题中，参数更新满足线性方程 $S\delta\theta = -\eta g$，这正是变分蒙特卡洛与神经网络量子态训练中常用的随机重构（SR）/量子自然梯度（QNG）更新。本文从经典部分开始，给出 KL 一阶项为零、二阶项等于 score 协方差的完整证明，并在清晰区分"概率输出"和"复振幅输出"的基础上，逐步引入 FS 距离、量子 Fisher 信息矩阵的规范不变定义及其与 SR 的几何推导。文末用两张对照表总结经典与量子概念的对应关系，以及常见优化算法与其隐含的度量/流形解释（包括 Adam/RMSProp 等对角近似）。
\end{abstract}

\tableofcontents

\section{经典概率分布}

\subsection{从KL 散度到Fisher信息矩阵}
设 $p_{\btheta}(x)$ 是由一组参数 $\btheta = (\theta^{1}, \dots, \theta^{d})$ 参数化的概率分布（离散与连续情形统一记作 $\sum_x$）。两分布之间的差异由 Kullback-Leibler (KL) 散度刻画：
\begin{equation}
D_{\mathrm{KL}} \left(p_{\btheta} \| p_{\btheta^\prime}\right) := \sum_x \, p_{\btheta}(x) \ln \frac{p_{\btheta}(x)}{p_{\btheta^\prime}(x)}
=\sum_x \, p_{\btheta}(x) \ln p_{\btheta}(x) - \sum_x \, p_{\btheta}(x) \ln p_{\btheta^\prime}(x).
\end{equation}
注意KL散度不对称且不满足三角不等式，所以严格来说它不是一种“距离”。经典信息几何把分布族 $\{p_{\btheta}\}$ 看作流形上的点集，我们仍希望在该流形上定义“距离”。一个自然思路是：当两分布非常接近时，用某种距离度量来近似 KL 散度。

考虑参数的微小变动 $\btheta^\prime = \btheta + \delta \btheta$，对KL散度做泰勒展开（即展开 $\ln p_{\btheta+\delta \btheta}(x)$）：
\begin{align*}
D_{\mathrm{KL}} \left(p_{\btheta} \| p_{\btheta + \delta \btheta}\right)&=\sum_x \, p_{\btheta}(x) \ln p_{\btheta}(x) - \sum_x \, p_{\btheta}(x) \ln p_{\btheta+\delta \btheta}(x)\\
&=\sum_x \, p_{\btheta}(x) \underbrace{\left[\ln p_{\btheta}(x) - \ln p_{\btheta}(x)\right]}_{=0} &\text{（零阶项）}\\
&- \delta \theta^i \underbrace{\sum_x p_{\btheta}(x) \partial_i \ln p_{\btheta}(x)}_{=0} &\text{（一阶项）}\\
&- \frac{1}{2} \delta \theta^i \delta \theta^j \underbrace{\sum_x p_{\btheta}(x) \partial_i \partial_j \ln p_{\btheta}(x)}_{:=-I_{ij}(\btheta)} &\text{（二阶项）}\\
&+ \mathrm{O}(\|\delta \btheta\|^3)
\end{align*}
其中
\begin{itemize}
    \item 零阶项为零（相同分布的KL散度为零）
    \item 一阶项为零：将概率归一化条件$\sum_x p_{\btheta}(x) = 1$ 对 $\theta^j$ 求导给出
\begin{equation}
    \partial_j \sum_x p_{\btheta}(x) \overset{\text{求导积分换序}}{=} \sum_x \partial_j p_{\btheta}(x) = \sum_x p_{\btheta}(x) \, \partial_j \ln p_{\btheta}(x) = 0.
    \label{eq:score_zero}
\end{equation}
即一阶项也为零。这里隐含了一个条件：对于 $p_{\btheta} (x)$，求导和积分可以换序，也即\textbf{正则化条件}。

    \item 二阶项：对式 \eqref{eq:score_zero} 再关于 $\theta^i$ 求导，
    \begin{align*}
    0 &= \partial_i \sum_x p_{\btheta}(x) \, \partial_j \ln p_{\btheta}(x)\\
    &= \sum_x \partial_i \bigl[p_{\btheta}(x) \, \partial_j \ln p_{\btheta}(x)\bigr]\\
    &= \sum_x \bigl[ p_{\btheta}(x) \, \partial_i \ln p_{\btheta}(x) \cdot \partial_j \ln p_{\btheta}(x) + p_{\btheta}(x) \, \partial_i \partial_j \ln p_{\btheta}(x) \bigr].
    \end{align*}
    这里隐含的条件是对于 $\partial_j \ln p_{\btheta} (x)$，求导和积分可以换序。
    移项得
    \begin{equation}
    \sum_x p_{\btheta}(x) \, \partial_i \ln p_{\btheta}(x) \cdot \partial_j \ln p_{\btheta}(x) = -\sum_x p_{\btheta}(x) \, \partial_i \partial_j \ln p_{\btheta}(x).
\end{equation}
    将式子左边定义为 \textbf{Fisher 信息矩阵}
    \begin{equation}
    \boxed{I_{ij}(\btheta) := \sum_x p_{\btheta}(x) \, \partial_i \ln p_{\btheta}(x) \cdot \partial_j \ln p_{\btheta}(x).}
    \label{eq:fisher_def}
\end{equation}
    观察到式子右边是$\ln p_{\btheta}(x)$对应的Hessian矩阵的负期望值，即
    \begin{equation}
        \boxed{\text{Fisher 信息矩阵} = - \mathbb{E}_{p_{\btheta}}[ \text{Hessian of } \ln p_{\btheta}(x) ]}
    \end{equation}
\end{itemize}

代入前述KL散度的泰勒展开，得到
\begin{equation}
\boxed{D_{\mathrm{KL}}(p_{\btheta} \| p_{\btheta+\delta\btheta}) \approx \frac{1}{2} \delta\theta^i \delta\theta^j I_{ij}(\btheta) = \frac{1}{2} \delta\btheta^\top I(\btheta) \, \delta\btheta.}
\end{equation}
这表明在局部我们可以用 $I_{ij}$ 作为距离来近似KL散度。这是KL散度诱导的一个黎曼度规，称为\textbf{Fisher-Rao 度规}，从而我们可以将分布们看作一个黎曼流形，并谈论其上的距离。

\begin{table}[ht]\centering
\caption{KL散度泰勒展开各阶项}
\begin{tabular}{c c p{6cm} c}
\toprule
\textbf{阶数} & \textbf{值} & \textbf{推导} & \textbf{条件} \\
\midrule
0 & $0$ & $D_{\mathrm{KL}}(p \| p) = 0$ & — \\
1 & $0$ & $\partial_i \bigl(\sum_x p_{\btheta}(x) - 1\bigr) = 0$ & 积分、求导可换序 \\
2 & $\frac{1}{2} \delta\btheta^\top I(\btheta) \, \delta\btheta$ & $\partial_i \partial_j \bigl(\sum_x p_{\btheta}(x) - 1\bigr) = 0$ & 积分、求导可换序 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Score 向量}
为了表述方便，我们定义 \textbf{score 向量}（对数似然梯度）
\[
s_i(x) := \partial_i \ln p_{\btheta}(x),
\]
并引入期望记号 $\mathbb{E}_{p_{\btheta}}[f(x)] := \sum_x p_{\btheta}(x) \, f(x)$。则前述各式可简写为：
\begin{itemize}
    \item 式 \eqref{eq:score_zero}：$\mathbb{E}[s_i] = 0$（score 均值为零）；
    \item 式 \eqref{eq:fisher_def}：$I_{ij} = \mathbb{E}[s_i \, s_j]$（Fisher 信息 = score 外积的期望）。
\end{itemize}
由于 $\mathbb{E}[s_i] = 0$，$I_{ij}$ 同时也是 score 的协方差矩阵：$I_{ij} = \mathrm{Cov}(s_i, s_j)$。

\textbf{注：}上述期望取自模型分布 $p_{\btheta}$。在机器学习中，损失函数的 Hessian $H_{ij} = \partial_i \partial_j \mathcal{L}$ 一般不等于 Fisher 信息矩阵。二者相等当且仅当：(1) 损失为负对数似然 $\mathcal{L} = -\ln p_{\btheta}(x)$；(2) 期望取自模型分布而非数据分布。

\subsection{流形上的最速下降：自然梯度}
考虑优化损失函数 $\mathcal{L}(\btheta)$，其梯度为 $g = \nabla_{\btheta} \mathcal{L}$。普通梯度下降以欧氏范数 $\|\delta\btheta\|_2$ 约束步长，但这不具备重参数化不变性。若改用 Fisher-Rao 度规 $\|\delta\btheta\|_I^2 = \delta\btheta^\top I(\btheta) \, \delta\btheta$ 约束步长，则最速下降问题为
\[
\max_{\delta\btheta} \; -g^\top \delta\btheta \quad \text{s.t.} \quad \delta\btheta^\top I(\btheta) \, \delta\btheta = \epsilon^2.
\]
用 Lagrange 乘子法，得
\begin{equation}
\boxed{I(\btheta) \, \delta\btheta = -\eta g, \quad \Rightarrow \quad \delta\btheta = -\eta I(\btheta)^{-1} g.}
\label{eq:natural_gradient}
\end{equation}
这就是 Amari 提出的\textbf{自然梯度}（Natural Gradient Descent, NGD）更新公式。

\section{从经典概率分布到量子态：量子 Fisher 信息矩阵}

\subsection{从概率模型类比到量子态}
经典概率模型的输出是实非负、归一化的概率分布 $p_{\btheta}(x)$。
量子模型（如参数化变分 Ansatz）的输出则是\textbf{复振幅} $\Psi_{\btheta}(x) := \langle x | \Psi(\btheta) \rangle$，概率通过 Born 规则给出：
\[
P(x) = \frac{|\Psi_{\btheta}(x)|^{2}}{\sum_{x'} |\Psi_{\btheta}(x')|^{2}} = \frac{|\Psi_{\btheta}(x)|^{2}}{\langle\Psi|\Psi\rangle}.
\]
类比经典的 score 向量 $s_i(x) = \partial_i \ln p_{\btheta}(x)$，我们定义\textbf{对数振幅导数}（复值 score）：
\begin{equation}
O_{i}(x) := \partial_{i} \ln \Psi_{\btheta}(x) = \frac{\partial_{i} \Psi_{\btheta}(x)}{\Psi_{\btheta}(x)}.
\end{equation}
自然地，尝试直接类比 Fisher 信息矩阵 $I_{ij} = \mathbb{E}_p[s_i s_j]$，定义“量子版本”：
\begin{equation}
\widetilde{S}_{ij} := \mathbb{E}_{P}[O_{i}^{*} O_{j}] = \sum_x P(x) \, O_{i}^{*}(x) \, O_{j}(x).
\end{equation}

然而，上述定义 $\widetilde{S}_{ij}$ 存在根本问题。量子态 $|\Psi\rangle$ 乘以任意非零复数 $c$ 不改变概率分布：$|c\Psi\rangle \sim |\Psi\rangle$。这种\textbf{规范冗余}（全局相位 + 整体幅值）意味着：
\begin{enumerate}
    \item 若参数变化 $\delta\btheta$ 只引起 $|\Psi\rangle \to e^{i\phi}|\Psi\rangle$ 这样的全局相位变化，概率分布完全不变，但 $\widetilde{S}_{ij}$ 却会给出非零值；
    \item 类似地，整体归一化变化 $|\Psi\rangle \to \lambda |\Psi\rangle$ 也不影响概率，却被 $\widetilde{S}_{ij}$ 误计入。
\end{enumerate}
问题的根源在于：$O_{i}$ 中包含了沿 $|\Psi\rangle$ 方向的分量，而这一分量对应的正是规范自由度。

解决方案是将 $O_{i}$ 中沿 $|\Psi\rangle$ 方向的分量扣除。注意到
\[
\mathbb{E}_{P}[O_{i}] = \sum_x P(x) \, \frac{\partial_{i} \Psi(x)}{\Psi(x)} = \frac{\langle \Psi | \partial_{i} \Psi \rangle}{\langle \Psi | \Psi \rangle}
\]
正是 $O_{i}$ 在 $|\Psi\rangle$ 方向的"均值"。定义\textbf{中心化的对数导数}：
\begin{equation}
\bar{O}_{i}(x) := O_{i}(x) - \mathbb{E}_{P}[O_{i}].
\end{equation}
\textbf{量子 Fisher 信息矩阵}（又称量子几何张量）定义为其协方差：
\begin{equation}
\boxed{S_{ij} := \mathbb{E}_{P}[\bar{O}_{i}^{*} \bar{O}_{j}] = \mathbb{E}_{P}[O_{i}^{*} O_{j}] - \mathbb{E}_{P}[O_{i}^{*}] \mathbb{E}_{P}[O_{j}].}
\label{eq:qgt_def}
\end{equation}
这一定义自动扣除了规范自由度：任何只引起 $|\Psi\rangle \to c|\Psi\rangle$ 的参数变化，其对应的 $\bar{O}_{i} = 0$，从而不贡献 $S_{ij}$。

在 braket 符号中，$S_{ij}$ 可写为
\begin{equation}
\boxed{S_{ij} = \frac{\langle \partial_{i} \Psi | (\mathbb{I} - \Pi_{\Psi}) | \partial_{j} \Psi \rangle}{\langle \Psi | \Psi \rangle}, \quad \Pi_{\Psi} = \frac{|\Psi\rangle \langle \Psi|}{\langle \Psi | \Psi \rangle},}
\end{equation}
其中投影算符 $\mathbb{I} - \Pi_{\Psi}$ 正是将 $|\partial_{i}\Psi\rangle$ 投影到 $|\Psi\rangle$ 的正交补空间。

\textbf{注}：在经典概率部分，Fisher 信息矩阵等于负对数似然 Hessian 的期望（式 \eqref{eq:fisher_def}）。但在量子态优化中，损失函数通常是能量期望值 $E(\btheta) = \langle\Psi|\hat{H}|\Psi\rangle / \langle\Psi|\Psi\rangle$，而非对数似然。因此能量的 Hessian $\partial_i \partial_j E$ 与量子 Fisher 信息矩阵 $S_{ij}$ 没有简单的等式关系。

\subsection{从 Fubini-Study 距离到量子 Fisher 信息矩阵}
类比经典的“KL 散度 $\Rightarrow$ Fisher 信息”，量子 Fisher 信息矩阵可从Fubini-Study (FS) 距离通过二阶展开得到。

对于两个态 $|\Psi\rangle$ 与 $|\Phi\rangle$，FS 距离定义为
\begin{equation}
\boxed{d_{\mathrm{FS}}(|\Psi\rangle, |\Phi\rangle) = \arccos \frac{|\langle \Psi | \Phi \rangle|}{\sqrt{\langle \Psi | \Psi \rangle \langle \Phi | \Phi \rangle}}.}
\end{equation}
显然 $d_{\mathrm{FS}}$ 对 $|\Psi\rangle \to c|\Psi\rangle$ 不变，因此是定义在射影空间 $\mathbb{CP}^{n-1}$ 上的距离。

考虑参数化态 $|\Psi(\btheta)\rangle$ 与 $|\Psi(\btheta+\delta\btheta)\rangle$ 的 FS 距离。记保真度
\[
\mathcal{F}(\delta\btheta) = \frac{|\langle \Psi(\btheta) | \Psi(\btheta+\delta\btheta) \rangle|^{2}}{\langle \Psi(\btheta) | \Psi(\btheta) \rangle \langle \Psi(\btheta+\delta\btheta) | \Psi(\btheta+\delta\btheta) \rangle}.
\]
在 $\delta\btheta = 0$ 处 $\mathcal{F}(0) = 1$。将 $\mathcal{F}$ 展开到二阶：
\[
\mathcal{F}(\delta\btheta) \approx 1 - \delta\theta^{i} \delta\theta^{j} g_{ij} + O(\|\delta\btheta\|^{3}),
\]
其中 $g_{ij} = \mathrm{Re}(S_{ij})$。由 $d_{\mathrm{FS}}^{2} \approx 2(1 - \sqrt{\mathcal{F}}) \approx g_{ij} \delta\theta^{i} \delta\theta^{j}$，得到
\begin{equation}
\boxed{\mathrm{d}s_{\mathrm{FS}}^{2} = g_{ij} \, \mathrm{d}\theta^{i} \mathrm{d}\theta^{j}.}
\end{equation}
这正是量子 Fisher 信息矩阵实部 $g_{ij}$ 作为 FS 度量的几何意义。

$S_{ij}$ 是个厄米的复值张量，实部对称、虚部反对称。将其分解为
\begin{equation}
\boxed{S_{ij} = g_{ij} + \frac{i}{2} F_{ij}, \quad g_{ij} = \mathrm{Re}(S_{ij}), \quad F_{ij} = 2\,\mathrm{Im}(S_{ij}).}
\end{equation}
\begin{itemize}
    \item \textbf{实部} $g_{ij} = g_{ji}$：对称半正定，是参数空间上的\textbf{Fubini-Study 度规}，度量两个态的距离；
    \item \textbf{虚部} $F_{ij} = -F_{ji}$：反对称，是\textbf{Berry 曲率}。
\end{itemize}

\subsection{流形上的最速下降：随机重构 / 量子自然梯度}
类比经典情形，以 FS 度规 $\|\delta\btheta\|_g^2 = \delta\btheta^\top S(\btheta) \, \delta\btheta$（取 $S$ 的实部或对称化版本）约束步长，最速下降问题为
\[
\max_{\delta\btheta} \; -g^\top \delta\btheta \quad \text{s.t.} \quad \delta\btheta^\top S(\btheta) \, \delta\btheta = \epsilon^2.
\]
用 Lagrange 乘子法，得
\begin{equation}
\boxed{S(\btheta) \, \delta\btheta = -\eta g, \quad \Rightarrow \quad \delta\btheta = -\eta S(\btheta)^{-1} g.}
\label{eq:sr_qng}
\end{equation}
这就是变分蒙特卡洛中的\textbf{随机重构}（Stochastic Reconfiguration, SR），也称\textbf{量子自然梯度}（Quantum Natural Gradient, QNG）。与经典自然梯度 $I(\btheta) \, \delta\btheta = -\eta g$ 形式完全相同，只是度量从 Fisher 信息矩阵换成了量子 Fisher 信息矩阵。

\begin{table}[ht]\centering
\caption{经典概率分布与量子态的对应关系\label{tab:compare1}}
\begin{tabular}{p{2.8cm} p{5.9cm} p{5.9cm}}
\toprule[1pt]
\textbf{概念} & \textbf{经典概率模型} & \textbf{量子态模型} \\
\midrule[0.5pt]
参数化对象 & 概率分布 $p_{\btheta}(x)$ & 态向量 $|\Psi(\btheta)\rangle$（等价类 $\sim c|\Psi\rangle$） \\
模型输出 & 概率 $p(x) \geq 0$，$\sum_x p = 1$ & 复振幅 $\Psi(x) \in \mathbb{C}$ \\
规范冗余 & 仅重参数化 & 重参数化 + 全局相位 + 整体尺度 \\
\midrule[0.3pt]
对数导数 & score $s_i = \partial_i \ln p$ & 复 score $O_i = \partial_i \ln \Psi$ \\
度量张量 & Fisher 信息矩阵

$I_{ij} = \mathrm{Cov}_p(s_i, s_j)$ & 量子 Fisher 信息矩阵

$S_{ij} = \mathrm{Cov}_P(O_i^*, O_j)$ \\
度量来源 & KL 散度的二阶展开：

$D_{\mathrm{KL}} \approx \frac{1}{2} \delta\btheta^\top I \, \delta\btheta$ & FS 距离的二阶展开：

$d_{\mathrm{FS}}^2 \approx \delta\btheta^\top \mathrm{Re}S \, \delta\btheta$ \\
额外结构 & 无（$I_{ij}$ 实对称） & Berry 曲率 $F_{ij} = 2\,\mathrm{Im}\, S_{ij}$（反对称） \\
\midrule[0.3pt]
最速下降 & 自然梯度 $I \, \delta\btheta = -\eta \nabla \mathcal{L}$ & SR/QNG $S \, \delta\btheta = -\eta \nabla E$ \\
\bottomrule[1pt]
\end{tabular}
\end{table}

\appendix
\section{优化算法的度量视角}
许多优化算法都可以写成“预条件梯度”(preconditioned gradient) 的统一形式：
\begin{equation}
\boxed{M_t\,\delta\btheta_t = -\eta_t\,\tilde g_t, \qquad \btheta_{t+1}\leftarrow \btheta_t+\delta\btheta_t,}
\label{eq:precond_form}
\end{equation}
其中 $t$是指第$t$步，$\tilde g_t$ 是用于更新的梯度信号（可以是原始梯度 $\tilde g_t=g_t$，也可以是动量/偏差校正后的版本），
$M_t\succeq 0$ 是预条件矩阵，可视为在参数空间上选取的局部度量：
\[
\|\delta\btheta\|_{M_t}^2 := \delta\btheta^\top M_t\,\delta\btheta.
\]
当 $\tilde g_t=g_t$ 且 $M_t$ 可逆时，$\delta\btheta_t=-\eta_t M_t^{-1}g_t$ 正对应于在约束 $\|\delta\btheta\|_{M_t}\le \epsilon$ 下的最速下降方向；
正文中的自然梯度取 $M_t=I(\btheta_t)$，SR/QNG 取 $M_t=\mathrm{Re}\,S(\btheta_t)$（或其对称化版本）。
若 $M_t$ 病态/半正定，实践中常用阻尼 $M_t\leftarrow M_t+\lambda\mathbb{I}$ 来稳定求解。

为避免记号歧义，以下约定：
\begin{itemize}
\item $u\odot v$ 表示逐元素乘法，$u^{\odot 2}:=u\odot u$；
\item 对矩阵 $A$，$\operatorname{diag}(A)$ 表示其对角向量，$\operatorname{Diag}(A)$ 表示保留其对角线的对角矩阵；
\item 对向量 $v$，$\operatorname{Diag}(v)$ 表示以 $v$ 为对角元的对角矩阵。
\end{itemize}

\begin{table}[ht]\centering
\caption{优化算法与预条件矩阵 $M_t$（度量/曲率视角）\label{tab:compare2}}
\begin{tabular}{p{3.1cm} p{5.3cm} p{5.9cm}}
\toprule[1pt]
\textbf{方法} & \textbf{预条件/度量 $M_t$} & \textbf{要点（与几何/曲率的关系）} \\
\midrule[0.5pt]
梯度下降 &
$M_t=\mathbb{I}$，$\tilde g_t=g_t$ &
欧氏度量：各方向同等尺度。\\

动量法 (Momentum) &
$M_t=\mathbb{I}$，$\tilde g_t=m_t$ &
欧氏度量 + 动量平滑（对梯度做一阶矩指数平均）。\\

牛顿法 (Newton) &
$M_t=\nabla_{\btheta}^{2}\mathcal{J}(\btheta_t)$，$\tilde g_t=g_t$ &
用 Hessian 表示局部二阶曲率；代价高且 $M_t$ 可能不定，需正则化/截断。\\
\midrule[0.3pt]
自然梯度 (NG) &
$M_t=I(\btheta_t)$，$\tilde g_t=g_t$（Fisher 信息矩阵） &
用 KL 散度二阶展开诱导的 Fisher--Rao 度量，实现重参数化不变的最速下降。\\

对角自然梯度 (Diag-NG) &
$M_t=\operatorname{Diag}(I(\btheta_t))$，$\tilde g_t=g_t$ &
\textbf{保留 Fisher 的对角线，忽略参数间相关性}；从 $O(d^2)$ 降到 $O(d)$。\\

AdaGrad &
$M_t \approx \operatorname{Diag}(G_t)^{1/2}$，$G_t=\sum_{\tau\le t} g_\tau^{\odot 2}$ &
对角自适应缩放：用历史梯度平方累积来调各维学习率。\\

RMSProp &
$M_t \approx \operatorname{Diag}(v_t)^{1/2}$，$v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^{\odot 2}$ &
AdaGrad 的指数滑动平均版本（更适合非平稳训练）。\\

Adam &
$M_t \approx \operatorname{Diag}(\hat v_t)^{1/2}$，$\tilde g_t=\hat m_t$ &
\textbf{对角自适应 + 动量 + 偏差校正。核心：$\hat v_t$ 可视为 Fisher 对角线的在线估计，从而 Adam 等价于“对角 Fisher 预条件”的自然梯度近似。}\\
\midrule[0.3pt]
随机重构 (SR/QNG) &
$M_t=\mathrm{Re}\,S(\btheta_t)$，$\tilde g_t=g_t$（取 $\mathcal{J}=E$） &
以 Fubini--Study 度量为约束得到的最速下降：$M_t\,\delta\btheta=-\eta\nabla_{\btheta}E$。\\
\bottomrule[1pt]
\end{tabular}
\end{table}

\paragraph{动量（作为 $\tilde g_t$）}动量法与 Adam 都引入梯度的一阶矩指数移动平均：
\[
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\qquad
\hat m_t=\frac{m_t}{1-\beta_1^t}.
\]
其中 $\hat m_t$ 是偏差校正后的版本（Adam 使用）。在统一形式 \eqref{eq:precond_form} 中，
动量对应于保持欧氏度量 $M_t=\mathbb{I}$，但把更新方向从 $g_t$ 替换为平滑后的 $\tilde g_t$。

\paragraph{Fisher 信息矩阵的\;\textbf{对角近似}（Adam $\leftrightarrow$ Diag-NG）}
下面给出把 Adam 与自然梯度联系起来所需的一个最小推导（与正文 Fisher 定义一致）。

设模型给出概率分布 $p_{\btheta}(x)$，并采用单样本的负对数似然损失
\[
\mathcal{L}(\btheta;x)=-\ln p_{\btheta}(x),\qquad g(\btheta;x)=\nabla_{\btheta}\mathcal{L}(\btheta;x).
\]
Fisher 信息矩阵（以模型分布取期望的“true Fisher”）定义为
\begin{equation}
I(\btheta)
:=\mathbb{E}_{x\sim p_{\btheta}}\!\Big[\nabla_{\btheta}\ln p_{\btheta}(x)\,\nabla_{\btheta}\ln p_{\btheta}(x)^\top\Big].
\label{eq:true_fisher}
\end{equation}
由于 $g(\btheta;x)=-\nabla_{\btheta}\ln p_{\btheta}(x)$，因此有
\begin{equation}
\boxed{I(\btheta)=\mathbb{E}_{x\sim p_{\btheta}}\!\big[g(\btheta;x)\,g(\btheta;x)^\top\big],\qquad
\operatorname{diag}(I(\btheta))=\mathbb{E}_{x\sim p_{\btheta}}\!\big[g(\btheta;x)^{\odot 2}\big].}
\label{eq:fisher_diag_grad2}
\end{equation}

在随机梯度/小批量场景中，我们用当前梯度 $g_t$ 来近似单样本 $g(\btheta;x)$，
并用指数滑动平均来估计逐元素二阶矩：
\[
v_t=\beta_2 v_{t-1}+(1-\beta_2)\,(g_t\odot g_t),
\qquad
\hat v_t=\frac{v_t}{1-\beta_2^t}.
\]
因此，在“负对数似然 + 以梯度平方估计期望”的近似下，
\begin{equation}
\boxed{\hat v_t \;\approx\; \mathrm{diag}(I(\btheta_t)).}
\label{eq:adam_diag_fisher_est}
\end{equation}
这一步就是关键：\textbf{Adam 选择了 Fisher 信息矩阵的对角近似（diagonal approximation of FIM）}，
并用 $\hat v_t$ 作为其在线估计。

Adam 的完整更新为
\[
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\quad
\hat m_t=\frac{m_t}{1-\beta_1^t},\quad
\btheta_{t+1}=\btheta_t-\alpha\,\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon},
\]
其中除法与开方均为逐元素运算。把它写回统一形式 \eqref{eq:precond_form}，可理解为
\[
\delta\btheta_t
= -\alpha\,\mathrm{diag}(\sqrt{\hat v_t}+\epsilon)^{-1}\,\hat m_t,
\]
即用一个\textbf{对角预条件矩阵}对梯度（动量信号）进行缩放。
结合 \eqref{eq:adam_diag_fisher_est}，可概括为
\begin{equation}
\boxed{\delta\btheta_t \;\approx\; -\alpha\,\mathrm{diag}(I(\btheta_t))^{-1/2}\,\hat m_t,}
\end{equation}
这就是“Adam = 对角 Fisher 预条件（再叠加动量与偏差校正）”的精确表述。

\paragraph{备注：Fisher、经验 Fisher 与 Hessian 的区别}
\begin{itemize}
\item 自然梯度严格使用的是 \eqref{eq:true_fisher} 中以 $p_{\btheta}$ 取期望的 true Fisher；
  实践中常用小批量梯度外积近似（经验 Fisher / empirical Fisher），它与 true Fisher 一般不完全相同。
\item Hessian $\nabla^2\mathcal{L}$ 与 Fisher $I(\btheta)$ 也一般不同；
  二者在特定条件下才可能一致或近似（例如匹配模型/大样本/正则性等假设）。
\end{itemize}

\begin{thebibliography}{99}
\bibitem{Amari1998} S.-i. Amari, \textit{Natural Gradient Works Efficiently in Learning}, Neural Computation \textbf{10}(2), 251–276 (1998).
\bibitem{Sorella1998} S. Sorella, \textit{Green Function Monte Carlo with Stochastic Reconfiguration}, Phys. Rev. Lett. \textbf{80}, 4558 (1998).
\bibitem{Sorella2005} S. Sorella, \textit{Wave function optimization in the variational Monte Carlo method}, Phys. Rev. B \textbf{71}, 241103(R) (2005).
\bibitem{Provost1980} J. Provost and G. Vallée, \textit{Riemannian structure on manifolds of quantum states}, Commun. Math. Phys. \textbf{76}, 289–301 (1980).
\bibitem{Izaac2019} J. Izaac, C. Wang, and Z. Wang, \textit{Quantum Natural Gradient}, arXiv:1811.08451 (2019).
\end{thebibliography}

\end{document}
